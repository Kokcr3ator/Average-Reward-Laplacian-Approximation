{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Import rlgrid modules\n",
    "from rlgrid import GridWorldEnv\n",
    "from rlgrid import (\n",
    "    uniform_policy,\n",
    "    random_policy,\n",
    "    random_deterministic_policy,\n",
    "    policy_induced_transition_matrix,\n",
    ")\n",
    "from rlgrid import (\n",
    "    simulate,\n",
    "    estimate_average_reward,\n",
    "    visualize_grid,\n",
    ")\n",
    "from rlgrid.utils import (\n",
    "    print_transition_info,\n",
    "    print_policy_info,\n",
    "    visualize_trajectory,\n",
    "    estimate_average_reward_running,\n",
    "    compute_average_reward_exact,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43e52b",
   "metadata": {},
   "source": [
    "## 1. Loading and Visualizing Grid Environments\n",
    "\n",
    "Grid environments are defined in `.txt` files using a simple format:\n",
    "- `.` = free cell (traversable)\n",
    "- `#` = wall (non-traversable)\n",
    "- `G` = goal cell (unique in the grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ca11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simple 5x5 environment\n",
    "env = GridWorldEnv.from_txt(\"../envs/simple_5x5.txt\", rng=rng)\n",
    "\n",
    "print(\"Grid layout:\")\n",
    "print(env)\n",
    "print()\n",
    "print(f\"Environment: {env}\")\n",
    "print(f\"Number of states: {env.n_states}\")\n",
    "print(f\"Number of actions: {env.n_actions}\")\n",
    "print(f\"Goal state: {env.goal_state}\")\n",
    "print(f\"Goal position: {env.goal_pos}\")\n",
    "print(f\"Grid size: {env.height} x {env.width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cc9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the grid\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "visualize_grid(env, ax=ax, show_state_indices=True, title=\"Simple 5x5 GridWorld\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize a more complex environment\n",
    "env_maze = GridWorldEnv.from_txt(\"../envs/larger_maze.txt\", rng=rng)\n",
    "\n",
    "print(\"Maze layout:\")\n",
    "print(env_maze)\n",
    "print()\n",
    "print(f\"Number of states: {env_maze.n_states}\")\n",
    "print(f\"Goal state: {env_maze.goal_state} at position {env_maze.goal_pos}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "visualize_grid(env_maze, ax=ax, show_state_indices=True, title=\"Larger Maze Environment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-to-position mappings\n",
    "print(\"State to Position mapping (simple 5x5):\")\n",
    "for state, pos in env.state_to_pos.items():\n",
    "    goal_marker = \" <- GOAL\" if state == env.goal_state else \"\"\n",
    "    print(f\"  State {state}: position {pos}{goal_marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a1754",
   "metadata": {},
   "source": [
    "## 2. Inspecting the Transition Kernel\n",
    "\n",
    "The transition kernel `P[s, a, s']` gives the probability of transitioning to state `s'` when taking action `a` in state `s`.\n",
    "\n",
    "Since our environment is **deterministic**, each `P[s, a, :]` has exactly one entry equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the transition kernel\n",
    "P = env.get_transition_kernel()\n",
    "print(f\"Transition kernel shape: {P.shape}\")\n",
    "print(f\"Expected: ({env.n_states}, {env.n_actions}, {env.n_states})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify deterministic transitions: each (s, a) should have exactly one next state\n",
    "print(\"Verifying deterministic transitions...\")\n",
    "is_deterministic = True\n",
    "for s in range(env.n_states):\n",
    "    for a in range(env.n_actions):\n",
    "        n_nonzero = np.count_nonzero(P[s, a, :])\n",
    "        if n_nonzero != 1:\n",
    "            print(f\"  State {s}, Action {a}: {n_nonzero} next states (should be 1)\")\n",
    "            is_deterministic = False\n",
    "\n",
    "if is_deterministic:\n",
    "    print(\"✓ All transitions are deterministic (exactly one next state per (s, a) pair)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect transitions from a specific state\n",
    "state_to_inspect = 0\n",
    "print_transition_info(env, state_to_inspect)\n",
    "print()\n",
    "\n",
    "# Also inspect transitions from the goal state\n",
    "print_transition_info(env, env.goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transition probabilities for a specific state and action\n",
    "state = 4  # Center state in simple 5x5\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(f\"Transition probabilities from state {state} ({env.state_to_pos[state]})\")\n",
    "\n",
    "for a, ax in enumerate(axes):\n",
    "    # Reshape for visualization\n",
    "    trans_probs = P[state, a, :]\n",
    "    \n",
    "    # Create a grid visualization\n",
    "    grid_probs = np.zeros((env.height, env.width))\n",
    "    for s_next, pos in env.state_to_pos.items():\n",
    "        grid_probs[pos[0], pos[1]] = trans_probs[s_next]\n",
    "    \n",
    "    # Mark walls as -1 for visualization\n",
    "    for r in range(env.height):\n",
    "        for c in range(env.width):\n",
    "            if env.grid[r][c] == '#':\n",
    "                grid_probs[r, c] = -0.2\n",
    "    \n",
    "    im = ax.imshow(grid_probs, cmap='Blues', vmin=-0.2, vmax=1)\n",
    "    ax.set_title(f\"Action: {env.ACTIONS[a]}\")\n",
    "    \n",
    "    # Mark the current state\n",
    "    row, col = env.state_to_pos[state]\n",
    "    ax.plot(col, row, 'ro', markersize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb8fa8",
   "metadata": {},
   "source": [
    "## 3. Defining Policies\n",
    "\n",
    "A policy π is a matrix of shape `(n_states, n_actions)` where `π[s, a]` is the probability of taking action `a` in state `s`.\n",
    "\n",
    "We have three policy constructors:\n",
    "1. **Uniform policy**: Equal probability for all actions\n",
    "2. **Random policy**: Random stochastic policy\n",
    "3. **Random deterministic policy**: One action per state, chosen randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different policies\n",
    "pi_uniform = uniform_policy(env)\n",
    "pi_random = random_policy(env, rng=rng)\n",
    "pi_det = random_deterministic_policy(env, rng=rng)\n",
    "\n",
    "print(\"Uniform Policy (all actions equally likely):\")\n",
    "print(pi_uniform)\n",
    "print()\n",
    "\n",
    "print(\"Random Stochastic Policy:\")\n",
    "print(pi_random.round(3))\n",
    "print()\n",
    "\n",
    "print(\"Random Deterministic Policy:\")\n",
    "print(pi_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify policies are valid (rows sum to 1)\n",
    "print(\"Verifying policy validity...\")\n",
    "for name, pi in [(\"Uniform\", pi_uniform), (\"Random\", pi_random), (\"Deterministic\", pi_det)]:\n",
    "    row_sums = pi.sum(axis=1)\n",
    "    all_valid = np.allclose(row_sums, 1.0)\n",
    "    print(f\"  {name}: rows sum to 1? {all_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db53891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed policy info\n",
    "print_policy_info(env, pi_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89dc9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute policy-induced transition matrix\n",
    "P_pi = policy_induced_transition_matrix(env, pi_uniform)\n",
    "\n",
    "print(f\"Policy-induced transition matrix shape: {P_pi.shape}\")\n",
    "print(f\"Expected: ({env.n_states}, {env.n_states})\")\n",
    "print()\n",
    "\n",
    "# Verify rows sum to 1\n",
    "row_sums = P_pi.sum(axis=1)\n",
    "print(f\"Row sums: {row_sums.round(6)}\")\n",
    "print(f\"All rows sum to 1? {np.allclose(row_sums, 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33214e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy-induced transition matrix\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "policies = [\n",
    "    (\"Uniform Policy\", pi_uniform),\n",
    "    (\"Random Stochastic Policy\", pi_random),\n",
    "    (\"Random Deterministic Policy\", pi_det),\n",
    "]\n",
    "\n",
    "for ax, (name, pi) in zip(axes, policies):\n",
    "    P_pi = policy_induced_transition_matrix(env, pi)\n",
    "    im = ax.imshow(P_pi, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Next State s'\")\n",
    "    ax.set_ylabel(\"Current State s\")\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bbb92",
   "metadata": {},
   "source": [
    "## 4. Simulating Trajectories\n",
    "\n",
    "Let's run simulations in the environment under different policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718891c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short simulation for demonstration\n",
    "T_short = 20\n",
    "states, actions, rewards = simulate(env, pi_uniform, T=T_short, start_state=0, rng=rng)\n",
    "\n",
    "print(f\"Simulation for {T_short} steps under uniform policy:\")\n",
    "print(f\"  States visited: {states}\")\n",
    "print(f\"  Actions taken: {actions}\")\n",
    "print(f\"  Rewards received: {rewards}\")\n",
    "print()\n",
    "print(f\"  Total reward: {rewards.sum()}\")\n",
    "print(f\"  Average reward: {rewards.mean():.4f}\")\n",
    "\n",
    "# Check for goal visits\n",
    "goal_visits = np.sum(rewards == 1)\n",
    "print(f\"  Goal reached {goal_visits} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b32f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate teleportation from goal\n",
    "print(\"Demonstrating teleportation from goal:\")\n",
    "print()\n",
    "\n",
    "# Find when we hit the goal\n",
    "for t in range(len(rewards)):\n",
    "    if rewards[t] == 1:\n",
    "        s = states[t]\n",
    "        a = actions[t]\n",
    "        s_next = states[t + 1]\n",
    "        \n",
    "        print(f\"  Step {t}: State {s} -> Action {env.ACTIONS[a]} -> \"\n",
    "              f\"Goal reached! (reward +1) -> Teleported to state {s_next}\")\n",
    "        print(f\"           Position {env.state_to_pos[s]} -> {env.state_to_pos[s_next]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4591433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a short trajectory\n",
    "visualize_trajectory(env, states, max_steps=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longer simulation to estimate average reward\n",
    "T_long = 10000\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, (name, pi) in enumerate([(\"Uniform\", pi_uniform), (\"Random Det.\", pi_det)]):\n",
    "    states, actions, rewards = simulate(env, pi, T=T_long, rng=rng)\n",
    "    \n",
    "    # Running average\n",
    "    running_avg = estimate_average_reward_running(rewards)\n",
    "    \n",
    "    # Plot running average\n",
    "    ax = axes[0, idx]\n",
    "    ax.plot(running_avg)\n",
    "    ax.axhline(y=running_avg[-1], color='r', linestyle='--', \n",
    "               label=f'Final avg: {running_avg[-1]:.4f}')\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('Running Average Reward')\n",
    "    ax.set_title(f'{name} Policy - Running Average Reward')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # State visitation histogram\n",
    "    ax = axes[1, idx]\n",
    "    ax.hist(states[:-1], bins=np.arange(env.n_states + 1) - 0.5, \n",
    "            edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=env.goal_state, color='g', linestyle='--', \n",
    "               linewidth=2, label=f'Goal state ({env.goal_state})')\n",
    "    ax.set_xlabel('State')\n",
    "    ax.set_ylabel('Visit count')\n",
    "    ax.set_title(f'{name} Policy - State Visitation')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53db7d",
   "metadata": {},
   "source": [
    "## 5. Average Reward Analysis\n",
    "\n",
    "In the average-reward setting, we care about the long-run average reward:\n",
    "\n",
    "$$\\rho(\\pi) = \\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=0}^{T-1} r_t$$\n",
    "\n",
    "For an ergodic MDP, this can be computed exactly using the stationary distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare simulation estimates with exact computation\n",
    "T = 100000\n",
    "burn_in = 1000\n",
    "\n",
    "print(\"Average Reward Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Policy':<25} {'Simulated':>15} {'Exact':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, pi in [(\"Uniform\", pi_uniform), \n",
    "                  (\"Random Stochastic\", pi_random),\n",
    "                  (\"Random Deterministic\", pi_det)]:\n",
    "    # Simulation estimate\n",
    "    _, _, rewards = simulate(env, pi, T=T, rng=rng)\n",
    "    sim_avg = estimate_average_reward(rewards, burn_in=burn_in)\n",
    "    \n",
    "    # Exact computation\n",
    "    exact_avg = compute_average_reward_exact(env, pi)\n",
    "    \n",
    "    print(f\"{name:<25} {sim_avg:>15.6f} {exact_avg:>15.6f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how average reward changes with different policies\n",
    "n_policies = 50\n",
    "avg_rewards = []\n",
    "\n",
    "for _ in range(n_policies):\n",
    "    pi = random_policy(env, rng=rng)\n",
    "    avg_r = compute_average_reward_exact(env, pi)\n",
    "    avg_rewards.append(avg_r)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(avg_rewards, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=np.mean(avg_rewards), color='r', linestyle='--', \n",
    "            label=f'Mean: {np.mean(avg_rewards):.4f}')\n",
    "plt.xlabel('Average Reward')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Distribution of Average Reward over {n_policies} Random Policies')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average reward statistics over {n_policies} random policies:\")\n",
    "print(f\"  Mean: {np.mean(avg_rewards):.6f}\")\n",
    "print(f\"  Std:  {np.std(avg_rewards):.6f}\")\n",
    "print(f\"  Min:  {np.min(avg_rewards):.6f}\")\n",
    "print(f\"  Max:  {np.max(avg_rewards):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd9fe5",
   "metadata": {},
   "source": [
    "## 6. Exploring Other Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the maze environment\n",
    "env_maze = GridWorldEnv.from_txt(\"../envs/maze_8x5.txt\", rng=rng)\n",
    "\n",
    "print(\"Maze 8x5:\")\n",
    "print(env_maze)\n",
    "print()\n",
    "print(f\"States: {env_maze.n_states}, Goal: {env_maze.goal_state}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "visualize_grid(env_maze, ax=ax, title=\"Maze 8x5 Environment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average reward in different environments\n",
    "environments = [\n",
    "    (\"Simple 5x5\", GridWorldEnv.from_txt(\"../envs/simple_5x5.txt\", rng=rng)),\n",
    "    (\"Maze 8x5\", GridWorldEnv.from_txt(\"../envs/maze_8x5.txt\", rng=rng)),\n",
    "    (\"Larger Maze\", GridWorldEnv.from_txt(\"../envs/larger_maze.txt\", rng=rng)),\n",
    "]\n",
    "\n",
    "print(\"Average Reward under Uniform Policy:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Environment':<20} {'States':>10} {'Avg Reward':>15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, e in environments:\n",
    "    pi = uniform_policy(e)\n",
    "    avg_r = compute_average_reward_exact(e, pi)\n",
    "    print(f\"{name:<20} {e.n_states:>10} {avg_r:>15.6f}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26189581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate in the larger maze\n",
    "env_large = GridWorldEnv.from_txt(\"../envs/larger_maze.txt\", rng=rng)\n",
    "pi = uniform_policy(env_large)\n",
    "\n",
    "T = 50000\n",
    "states, actions, rewards = simulate(env_large, pi, T=T, rng=rng)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Running average\n",
    "running_avg = estimate_average_reward_running(rewards)\n",
    "axes[0].plot(running_avg)\n",
    "axes[0].axhline(y=compute_average_reward_exact(env_large, pi), \n",
    "                color='r', linestyle='--', label='Exact')\n",
    "axes[0].set_xlabel('Time step')\n",
    "axes[0].set_ylabel('Running Average Reward')\n",
    "axes[0].set_title('Larger Maze - Running Average Reward')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# State visitation\n",
    "axes[1].hist(states[:-1], bins=np.arange(env_large.n_states + 1) - 0.5, \n",
    "             edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('State')\n",
    "axes[1].set_ylabel('Visit count')\n",
    "axes[1].set_title('Larger Maze - State Visitation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a85a3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Environment Loading**: Grid environments are easily defined in `.txt` files and loaded with `GridWorldEnv.from_txt()`\n",
    "\n",
    "2. **Transition Kernel**: The deterministic transition kernel `P[s, a, s']` is correctly built with exactly one next state per (state, action) pair\n",
    "\n",
    "3. **Policies**: We can define uniform, random stochastic, and random deterministic policies\n",
    "\n",
    "4. **Policy-Induced Transitions**: The matrix `P_π` correctly captures state-to-state transition probabilities under a policy\n",
    "\n",
    "5. **Simulation**: Trajectories can be simulated with teleportation from goal working correctly\n",
    "\n",
    "6. **Average Reward**: Both simulation-based and exact computation of average reward are available\n",
    "\n",
    "This provides a solid foundation for testing theoretical bounds in average-reward reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
